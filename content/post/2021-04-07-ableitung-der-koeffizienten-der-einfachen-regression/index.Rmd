---
title: Ableitung der Koeffizienten der einfachen Regression
author: Sebastian Sauer
date: '2021-04-07'
slug: ableitung-der-koeffizienten-der-einfachen-regression
categories:
  - statistics
tags:
  - regression
  - modelling
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
---



```{r knitr-setup, echo = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```


```{r}
library(tidyverse)
```



# Was ist die Regression?


In diesem Post geht es um die einfache Regression (d.h. mit einem Prädiktor); genauer gesagt um die Frage, wie man auf die Formeln der Koeffizienten der einfachen Regression kommt. 


Gehen wir von einigen zweidimensionalen Datenpunkten aus, die zu einem Phänomen gemessen wurden: ${(x_1, y_1), (x_2, y_2), \ldots, (x_n,y_)}$.

Gehen wir weiter davon aus, dass der Zusammenhang *in Wirklichkeit* von einer Geraden erklärt werden kann:

$$\hat{y}=b_0 + b_1x.$$

Eine Gerade kann durch zwei Koeffizienten definiert werden, den Achsenabschnitt $b_0$ und die Steigung $b_1$.

Für jeden Datenpunkt $y_i$ können wir die Abweichung (den Fehler) zum vorhergesagten Wert $\hat{y_i}$ ausrechnen:

$$e_i = y_i - \hat{y_i}$$

Nennen wir die Summe der quadrierten Fehler $e_i$ im Folgenden $QS$ (für Quadratsumme):



\begin{align*}
QS &= \sum_{i=1}^{n}e_i^2\\
QS &=  \sum_{i=1}^{n}(y_i - \hat{y_i})^2\\
QS &=  \sum_{i=1}^{n}(y_i - b_0 - b_1x_i)^2 
\end{align*}

Für eine übersichtlichere Notation wird der Index am Summenzeichen im Folgenden weggelassen.

Betrachten wir das an einem Datenbeispiel:

```{r}
d <- mtcars
lm1 <- lm(mpg ~ hp, data = d)

d <-
  d %>% 
  mutate(pred = predict(lm1),
         resid = residuals(lm1))

```

Jetzt visualisieren wir die Residuen (Abweichungen, Fehler):

```{r}
ggplot(d, 
       aes(x = hp, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "dodgerblue") +
  geom_segment(aes(xend = hp, yend = pred), alpha = .5) +
  geom_point() +
  geom_hline(yintercept = mean(d$mpg), 
             color = "grey60", linetype = "dashed") +
  geom_vline(xintercept = mean(d$hp), 
             color = "grey60", linetype = "dashed") + 
  annotate("point", x = mean(d$hp), y = mean(d$mpg),
           size = 5, color = "red", alpha = .5)
```

Die blaue Gerade ist die Regressionsgerade; die vertikalen Balken stellen die Residuen dar und die gestrichelten Linien repräsentieren jeweils die Mittelwerte von `hp` bzw. `mpg`. Man beachte, dass der Schnittpunkt der Mittelwertslinien auf der Regressionsgeraden liegt.



# Wie findet man die Regressionsgerade?


Um die Koeffizienten der Regressionsgeraden zu bestimmen, können wir die *Methode der kleinsten Quadrate (least squares)* verwenden. Diese Methode gibt uns diejenigen Koeffizienten der Regressionsgeraden (also $b_0$ und $b_1$), die die *Quadratsumme (QS) der Residuen* $e_i$ minimieren.

Um die Quadratsumme zu minimieren, bilden wir jeweils die erste parzielle Ableitung und setzen diese Null. Anschließend löst man nach dem gesuchten Koeffizienten auf. Beginnen wir mit $b_0$.

## $b_0$

\begin{align*}
\frac{\partial QS}{\partial b_0} &= 2 \sum(y_i - b_0 - b_1x_i)(-1) \\
&= -2 \sum y_i + 2 \sum b_0 + 2 \sum b_1 x_i \\
&= 2(-\sum y_i + nb_0 + b1 \sum x_i) = 0 \\
&= -\sum y_i + nb_0 + b1 \sum x_i = 0 \\
 n b_0&= \sum y_i - b_1 \sum x_i \\
b_0 &= \frac{\sum y_i - b_1 \sum x_i}{n} \\
&= n^{-1}\sum y_i - n^{-1}b_1 \sum x_i \\
&= \bar{y} - b_1 {x}
\end{align*}


Das Ergebnis zeigt auch, dass der Punkt $(\bar{x}, \bar{y})$ auf der Regressionsgerade liegt.


## $b_1$

Analog verfahren wir für den zweiten Koeffizienten, $b_1$. Wieder bilden wir die parzielle Ableitung, setzen diese Null und lösen nach dem gesuchten Koeffizienten auf. Das Nachdifferenzieren (Kettenregel) liefert allerdings ein anderes Ergebnis.

\begin{align*}
\frac{\partial QS}{\partial b_1} &= 2 \sum(y_i - b_0 - b_1x_i)(-x) \\
&=2(-\sum y_i x_i + b_0\sum x_i + b_1\sum x_i^2)\\
&=-2 \sum(x_iy_i - b_0x_i - b_1x_i^2) 
\end{align*}

Jetzt setzen wir $b_0$ in die letzte Gleichung ein und setzen diese Null:

\begin{align*}
&= -2 \sum(x_i y_i - (\bar{y} - b_1\bar{x})x_i - b_1x_i^2) \\
&= -2 \sum(x_iy_i - x_i\bar{y} + b_1 x_i \bar{x} - b_1x_i2) = 0 
\end{align*}

die $-2$ kürzt sich im Folgenden heraus. Um nach $b_1$ aufzulösen, gruppieren wir die Terme in zwei Hälften: Die mit und die ohne $b_1$:

$$
(x_i y_i - x_i\bar{y}) - b_1 \sum(x_i^2 - x_i \bar{x}) = 0
$$

Die Terme ohne $b_1$ wandern auf die rechte Seite der Gleichung:

\begin{align*}
b_1 \sum(x_i^2 - x_i \bar{x}) &= \sum(x_i y_i - x_i\bar{y}) \\
b_1 &= \frac{\sum x_i y_i - x_i \bar{y}}{\sum x_i^2 - x_i \bar{x}} = \\
&= \frac{x_i y_i  - n\bar{x} \bar{y}}{\sum x_i^2 - n \bar{x}\bar{x}}
\end{align*}


## Weitere Umformung von $b_1$

Damit haben wir nach $b_1$ aufgelöst. Allerdings kann man diesen letzten Ausdruck noch in bekanntere Terme umformulieren. Dazu sind zwei Hilfsüberlegungen nützlich:


### Hilfsterm 1

$$
\sum (\bar{x}^2 - x_i \bar{x}) = n\bar{x}^2 - n \bar{x} \bar{x} = 0
$$


### Hilfsterm 2

$$
\sum (\bar{x}\bar{y} - y_i \bar{x}) = n \bar{x} \bar{y} - n \bar{x} \bar{y} = 0
$$


Setzen wir diese Hilfsterme in obige Gleichung ein und zwar Hilfsterm 1 in den Nenner und Hilfsterm 2 in den Zähler. Das "dürfen" wir, da die Hilfsterme ja jeweils Null sind.


\begin{align*}
b_1 &= \frac{(\sum x_i y_i - x_i \bar{y}) + \sum (\bar{x}\bar{y} - y_i \bar{x})}{(\sum x_i^2 - x_i \bar{x}) + \sum (\bar{x}^2 - x_i \bar{x})}
\end{align*}


### Umformung des Zählers

Im Folgenden betrachten wir der Einfachheit halber nur den Zähler.

\begin{align*}
b_1 &= \sum x_i y_i - \sum x_i \bar{y} + \sum \bar{x} \bar{y} - \sum y_i \bar{x} \\
&= \sum(x_i y_i - x_i \bar{y} + \bar{x}\bar{y} - y_i \bar{x}) \\
&= \sum(x_i(y_i - \bar{y}) + \bar{x}(y_i - \bar{y})) \\
&= \sum \left( (x_i - \bar{x})(y_i - \bar{y}) \right)
\end{align*}

Multipliziert man nun Zähler und Nenner jeweils (und damit neutral) mit $1/n$, so erhält man

$$
b1 = \frac{cov(x,y)}{s^2(x)}.
$$

Dabei steht $s^2(x)$ für die Varianz von $x$ und $cov$ für die Kovarianz. 

## Noch weitere Umformung von $b_1$

\begin{align*}
b1 &= \frac{cov(x,y)}{s^2(x)} \\
&= \frac{cov(x,y)}{s^2(x)} \cdot \frac{s(y)}{s(y)} \\
&= \frac{cov(x,y)}{s(x)s(y)}  \cdot \frac{s(y)}{s(x)} \\
&= cor(x,y) \cdot \frac{s(y)}{s(x)}
\end{align*}


# Fazit


In diesem Post ging es um die Ableitung der Formeln der Regressionskoeffizienten, zumindest der einfachen Regression. Dabei wurden grundlagende Algebra und Parzielle Ableitungen verwendet. Wir haben also bewiesen, dass die Regressionskoeffizienten die Form haben, die sie haben.

Für die Ableitung der multiplen Regression sind Ansätze auf Basis der linearen Algebra praktischer.


# Reproduzierbarkeit

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
devtools::session_info()


