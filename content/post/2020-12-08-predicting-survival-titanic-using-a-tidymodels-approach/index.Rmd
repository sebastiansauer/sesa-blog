---
title: "Predicting survival (Titanic) using a tidymodels approach"
author: "Sebastian Sauer"
date: '2020-12-08'
draft: false
slug: predicting-survival-titanic-using-a-tidymodels-approach
categories:
  - rstats
tags:
  - prediction
output:
  blogdown::html_page:
    
    toc: true
    number_sections: TRUE
editor_options: 
  chunk_output_type: console
---



```{r knitr-setup, echo = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```


# Motivation

Let us predict survival at the [Titanic disaster](https://www.kaggle.com/c/titanic) using the Kaggle competition data, download from [here](https://www.kaggle.com/c/titanic/data).

We will make use of the [tidymodels](https://www.tidymodels.org/start/models/) approach.



# Load packages

```{r load-libs, message = FALSE, warning = FALSE}
library(tidyverse)  # data wrangling
library(tidymodels)  # modelling
library(broom)  # tidy model output
library(DescTools)  # Pseudo R^2
library(skimr)  # overview on descriptives
library(gt) # nice tables
```




# Load data



```{r}
traindata_url <- "https://raw.githubusercontent.com/sebastiansauer/Statistiklehre/main/data/titanic/train.csv"
testdata_url <- "https://raw.githubusercontent.com/sebastiansauer/Statistiklehre/main/data/titanic/test.csv" 
```




```{r}
train <- read_csv(traindata_url)
test <- read_csv(testdata_url)
```



```{r}
glimpse(train)
```

The train data set lacks the "output" column, `Survived`,  which makes sense.

```{r}
dim(train)
dim(test)
```



# Data preprocessing


## Classification models


We will transform outcome variable from numeric to factor, which is necessary for many classification models. 

```{r}
train %>% 
  summarise(Survived_n_dist = n_distinct(Survived))
```


```{r}
head(train$Survived)
```

```{r}
train <- train %>% 
  mutate(Survived = as.factor(Survived))
```



```{r}
head(train$Survived)
```




## Add Survived to test set

```{r}
test <- 
  test %>% 
  mutate(Survived = NA)
```


## Merge train and test

```{r}
data <-
  train %>% 
  bind_rows(test)
```



```{r}
split_titanic <- initial_time_split(data = data, prop = 891/1309)
train2 <- training(split_titanic)
test2 <- testing(split_titanic)
```




## NAs


```{r}
skim(train)
```

OK, for age, cabin, and embarked, we have some NAs. No NAs for `Survived` (outcome).

# Define recipe

```{r}
titanic_recipe <- 
  
  # define model formula:
  recipe(Survived ~ Pclass + Age + SibSp + Parch + Fare, data = train) %>%
  
  # Use "ID" etc as ID, not as predictor:
  #update_role(Ticket, Cabin, Name, PassengerId, Embarked,  
   #         new_role = "ID") %>%   
  
  update_role( Pclass , Age , SibSp , Parch , Fare, new_role="predictor") %>%
  
  # Convert outcome variable from numeric to factor, to indicate classification
  # step_num2factor(Survived, levels = c("0", "1")) %>%   # not working, #todo
  
  # impute missing values:
  step_knnimpute(all_predictors(), neighbors = 3) %>%  
  
  # remove these vars:
  #step_rm(Ticket, Cabin, Name, PassengerId, Embarked) %>%   
  
  # convert character and factor type variables into dummy variables:
  step_dummy(all_nominal(), -all_outcomes()) %>%   
  
  # exclude near zero variance predictors:
  step_nzv(all_predictors()) %>%  
  
   # exclude highly correlated vars:
  step_corr(all_predictors()) %>% 
  
  # center (set mean to zero):
  step_center(all_predictors(), -all_outcomes()) %>%  
  
  # set sd=1 
  step_scale(all_predictors(), -all_outcomes())   

```


```{r}
summary(titanic_recipe)
```




That is only the recipe, not the data (however the data are stored in the object too).


```{r}
titanic_recipe
```


# Prepare (`prep()`) the recipe


```{r}
titanic_recipe_prepped <-
  titanic_recipe %>% 
  prep(verbose = TRUE) 
```



```{r}
titanic_recipe_prepped
```

```{r}
summary(titanic_recipe_prepped)
```


Note, from `help(prep)`: 

>     training: A data frame or tibble that will be used to estimate parameters for preprocessing.



# Difference between a recipe and a prepped recipe

Compare:

```{r}
titanic_recipe
```



With:

```{r}
titanic_recipe_prepped
```


For example, we see that the mean values for `Centering` have been `trained` during the `prep()` step. Note that the steps are still *not*  applied to the data set. That is what `bake()` is for. AFAIK, `train()` will do that automatically, so no action needed for the test set in that regard.


See [here](https://www.tidymodels.org/find/recipes/) an complete of currently available preprocessing steps -- more than 100 as to date.


# Check prepped data set

Squize the juice out of the prepared recipe ...

```{r}
juice(titanic_recipe_prepped) %>% 
  head()
```


Are there any NAs in the prepped data set?

```{r}
juice(titanic_recipe_prepped) %>% 
  map(~ sum(is.na(.x)))
```


# Juice the train data


```{r}
train_juice <- juice(titanic_recipe_prepped)
```



# Bake recipe to train data


```{r}
bake(titanic_recipe_prepped, new_data = NULL)
```




# Bake recipe to test data

WARNING #todo

```{r error = TRUE}
test_baked <-
  titanic_recipe_prepped %>% 
  bake(new_data = test)
```








# Model 1: glm




## Define model

```{r}
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```


```{r}
lr_mod
```



## Define modelling workflow


```{r}
titanic_wf1 <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(titanic_recipe)
```


```{r}
titanic_wf1
```







## Fit model


```{r fit-glm1}
titanic_fit_lr <-
  titanic_wf1 %>% 
  fit(data = train)
```





Model performance on *train* data:


```{r}
titanic_fit_lr %>% 
  pull_workflow_fit() %>% 
  tidy()
```

```{r}
titanic_fit_lr %>% 
  pull_workflow_fit() %>% 
  glance()
```


```{r}
titanic_fit_lr %>% 
  pull_workflow_fit() %>% 
  summary() 
```



```{r}
pull_workflow_prepped_recipe(titanic_fit_lr)
```


Hm, for some reason, not working. #todo

```{r}
pull_workflow_fit %>%
  PseudoR2()
```

## Extract fit    


```{r}
titanic_fit_lr %>% 
  pluck("fit")
```






## Predict test data





```{r glm1-pred}
test_with_pred <-
  test %>% 
  bind_cols(predict(titanic_fit_lr, test_baked)) %>% 
  rename(pred_glm1 = .pred_class)
```

```{r}
glimpse(test_with_pred)
```






## Save predictions to disk

```{r eval = FALSE}
test_with_pred %>% 
  select(PassengerId, Survived = pred_glm1) %>% 
  write_csv(file = "titanic-glm1.csv")
```



# Model 2: Random forests 1 (without tuning, w/o cv)


## Define model

```{r rf1-mod}
rf_mod1 <- 
  rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```

```{r}
rf_mod1
```





## Define workflow


```{r rf1-workflow}
rf_workflow1 <- 
  workflow() %>% 
  add_model(rf_mod1) %>% 
  add_recipe(titanic_recipe)
```





## Fit model

```{r fit-rf1}
rf_fit1 <- 
  rf_workflow1 %>% 
  fit(data = train)
```


```{r}
rf_fit1
```





## Predict test data


```{r rf1-pred}
rf1_preds <- 
  rf_fit1 %>% 
  predict(new_data = test_baked) %>% 
  bind_cols(test) %>% 
  select(-Survived) %>% 
  select(PassengerId, Survived = .pred_class)
```




```{r}
glimpse(rf1_preds)
```



## Save predictions to disk

```{r eval = FALSE}
pred_rf1 %>% 
  write_csv(file = "titanic-rf1.csv")
```







# Model 3: Random forests 2 (with tuning and with CV)


## Detect cores

```{r}
cores <- parallel::detectCores()
cores
```




## Define model

```{r def-rf2}
rf_mod2 <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```

```{r}
rf_mod2
```


Parameters that can be tuned:


```{r}
rf_mod2 %>%    
  parameters()  
```


## Define cross validation scheme


```{r rf2-vfold-cv}
train_cv <- vfold_cv(train, v = 10)
```

10-times CV:

```{r}
train_cv
```




## Define workflow


```{r wf-rf2}
rf_workflow2 <- 
  workflow() %>% 
  add_model(rf_mod2) %>% 
  add_recipe(titanic_recipe)
```


```{r}
rf_workflow2
```






## Define and run tune grid


```{r rf2-tune-grid}
set.seed(42)

t1 <- Sys.time()
rf_res2 <- 
  rf_workflow2 %>% 
  tune_grid(resamples = train_cv,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
t2 <- Sys.time()

t2 - t1
```

It may be worthwhile to save the object to disk, in order to save computation time:

```{r}
saveRDS(rf_res2, file = "rf_res2.rds")
```


## View results


```{r}
rf_res2 %>% 
  collect_metrics()
```



```{r}
rf_res2 %>% 
  show_best(metric = "roc_auc")
```




```{r}
autoplot(rf_res2)
```




## Get best model

```{r r2f-get-best}
rf_best2 <- 
  rf_res2 %>% 
  select_best(metric = "roc_auc")
rf_best2
```




## Final fit


```{r rf2-finalize-wf}
rf_final_wf2 <- 
  rf_workflow2 %>% 
  finalize_workflow(rf_best2)
```

So, here is the best model:

```{r}
rf_final_wf2
```


Let us fit this model to the train data:

```{r rf-fit-final-model}
last_rf_mod2 <- 
  rf_final_wf2 %>% 
  fit(data = train)
```


```{r}
last_rf_mod2
```


## Legacy code, do not run

```{r eval = FALSE}
  # rand_forest(mtry = rf_best2[["mtry"]][1], 
  #             min_n = rf_best2[["min_n"]][1], 
  #             trees = 1000) %>% 
  # set_engine("ranger", 
  #            num.threads = cores, 
  #            importance = "impurity") %>% 
  # set_mode("classification")
```



Update workflow with best fit parameters

```{r eval = FALSE}
# last_rf_workflow2 <- 
#   rf_workflow2 %>% 
#   update_model(last_rf_mod)
```

## Fit final workflow


```{r}
set.seed(345)
last_rf_fit2 <- 
  last_rf_mod2 %>% 
  last_fit(split_titanic)
```



## Predict test data

```{r rf2-pred}
rf2_preds <- 
last_rf_fit2 %>% 
  collect_predictions() %>% 
  select(-Survived) %>% 
  select(PassengerID = .row, Survived = .pred_class) 
```


```{r}
glimpse(rf2_preds)
```



## Save predictions to disk

```{r eval = FALSE}
rf2_preds %>% 
  write_csv(file = "titanic-rf2.csv")
```








# Model 4: Boosted trees (with tuning)






## Define model

```{r def-boost-mod}
boost_mod <- 
  boost_tree(mtry = tune(), 
             min_n = tune(), 
             learn_rate = tune(),
             tree_depth = tune()) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")
```

```{r}
boost_mod
```

Translate to `xgboost()` specs:


```{r}
translate(boost_mod)
```

Why does it say `ntread = 1`? Should be more cores, right? #todo

Parameters that can be tuned:


```{r}
boost_mod %>%    
  parameters()  
```



## Define workflow


```{r def-boost-wf}
boost_wf <- 
  workflow() %>% 
  add_model(boost_mod) %>% 
  add_recipe(titanic_recipe)
```


```{r}
boost_wf
```


## Define analysis and validation (oob) set


```{r}
set.seed(234)
val_set <- validation_split(train, 
                            strata = Survived, 
                            prop = 0.80)
```





## Define tune grid


```{r boost-tune-grid}
set.seed(42)

t1 <- Sys.time()
boost_fit <- 
  boost_wf %>% 
  tune_grid(val_set,
            grid = 100,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
  t2 <- Sys.time()

t2 - t1
```



```{r}
boost_fit %>% 
  show_best(metric = "roc_auc")
```




```{r}
autoplot(boost_fit)
```




## Get best model

```{r boost-get-best-model}
boost_best <- 
  boost_fit %>% 
  select_best(metric = "roc_auc")
boost_best
```



## Final fit


```{r boost-finalize-wf}
boost_final_wf <- 
  boost_wf %>% 
  finalize_workflow(boost_best)
```


## Legacy code, do not run


```{r}
# last_boost_mod <- 
#   boost_tree(mtry = boost_best[["mtry"]][1], 
#              min_n = boost_best[["min_n"]][1], 
#              tree_depth = boost_best[["tree_depth"]][1], 
#              trees = 1000) %>% 
#   set_engine("xgboost", 
#              num.threads = cores, 
#              importance = "impurity") %>% 
#   set_mode("classification")
```



Update workflow with best fit parameters

```{r}
# last_boost_workflow <- 
#   boost_wf %>% 
#   update_model(last_boost_mod)
```



```{r}
# set.seed(345)
# last_boost_fit <- 
#   last_boost_workflow %>% 
#   fit(data = train)
```



## Fit final workflow


```{r boost-last-fit}
set.seed(345)
last_boost_results <- 
  boost_final_wf %>% 
  last_fit(split_titanic)
```



```{r}
last_boost_results
```

## Predict test data


```{r boost-preds}
boost_preds <- 
  last_boost_results %>% 
  collect_predictions() %>% 
  select(-Survived) %>% 
  select(PassengerID = .row, Survived = .pred_class)
```





```{r}
glimpse(boost_preds)
```




## Save predictions to disk

```{r eval = FALSE}
boost_preds %>% 
  write_csv(file = "titanic-boost.csv")
```





# Conclusions


`tidymodels` appear to be a nice framework, but not without some difficulties. I needed some time to wrap my head around it; in effect, I am still picking it up.

Computing many models in the way I worked here is bound to end up in disaster: Too many object to rename and to keep track off. There are better ways needed such as [this one](https://www.brodrigues.co/blog/2020-03-08-tidymodels/). Alternatively, one might seek shelter in object oriented programming, which is beautiful in theory, but not without complexites. See [mlr3](https://mlr3book.mlr-org.com/) for a great implementation.    


# Similar work

Here is a quite similar [case study on Kaggle](https://www.kaggle.com/stautxie/titanic-tidymodels-v1).

# More advanced work

[This is a great case study on tidymodels](https://www.brodrigues.co/blog/2020-03-08-tidymodels/), by Bruno Rodrigues. Similarly, by the same author, [this post](https://www.brodrigues.co/blog/2018-11-25-tidy_cv/).











# Reproducibility

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
devtools::session_info()
```








