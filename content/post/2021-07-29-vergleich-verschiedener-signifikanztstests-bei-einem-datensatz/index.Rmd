---
title: Vergleich verschiedener Signifikanztstests bei einem Datensatz
author: Sebastian Sauer
date: '2021-07-29'
slug: vergleich-verschiedener-signifikanztstests-bei-einem-datensatz
draft: no
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
---



# Setup

```{r knitr-setup, echo = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```




```{r}
library(tidyverse)
library(mosaic)
library(testthat)
library(magrittr)
library(sessioninfo)
```



# Fallbeispiel

Im Statistikskript der [FOM-Hochschule](https://www.fom.de/) (entwickelt vom [ifes](https://www.fom.de/forschung/institute/ifes.html)) wird folgendes Beispiel zur Einführung von simulationsbasierter Inferenz verwendet:


>    Im Rahmen einer Sonderveranstaltung der FOM Dortmund (6.10.2016) 102 und Münster (9.11.2017) tippten von n = 34 Teilnehmer*innen x = 12 im Rahmen eines Dreieckstest auf die richtige Probe, d. h. das andere Bier.


Wandeln wir die Daten leicht ab (dann geht es schöner auf).

Also sagen wir:

```{r}
n <- 36
x <- 14
prob_null <- 1/3
```


Aus Gründen der Konvergenz vergrößern wir die Stichprobe noch einmal:

```{r}
n2 <- n * 10
x2 <- x * 10
```



# Datensatz

Erstellen wir eine Funktion, die einen Datensatz, in Abhängigkeit gewählter Parameter $n$ und $x$ erstellt.


## Funktion, um Daten zu simulieren

```{r}
bier_data <- function(n, x) {
  # n: sample size
  # x: successes
  # This function generates data for Bernoulli chain data
  
  
  out <-
    tibble(id = 1:n,
           success = c(rep.int(1, x), rep.int(0, n-x)))

  return(out)

}
```


```{r}
d <- bier_data(n = n, x = x)

glimpse(d)
```



```{r}
d2 <- bier_data(n = n2, x = x2)
```


## Unit Testing

Im Sinne des [Test-Driven-Designs](https://en.wikipedia.org/wiki/Test-driven_development): Schreiben wir zuerst einen Unit Test (oder mehrere) für die geplante (noch nicht geschriebene!) Funktion zur Erstellung der (simulierten) Daten.






```{r}
test_that("Datensatz in Abhängigkeit von n und x erstellt",
          {
            
            expect_equivalent(nrow(bier_data(34, 12)), 34)
            expect_equivalent(sum(bier_data(34,12)$success), 12)
            
          })
```




# Signifikanz-Tests

$$H_0: \pi = 1/3$$
$$H_A: \pi \ne 1/3$$

Wir testen also ungerichtet.

$\pi=1/3$ rührt daher, dass es drei Optionen gibt und nur eine davon ist ein Treffer.


## Simulationsbasierte Inferenz (SBI)

### Kleine Stichprobe

```{r}
set.seed(1896) 
Nullvtlg <- do(10000) * rflip(n = n, prob = prob_null)

glimpse(Nullvtlg)
```


Berechnen wir $\bar{\pi}$:


```{r}
propdach <- sum(d$success) / nrow(d)
propdach
```


Mit Methoden des `tidyverse` ist es umständlicher, `propdach` zu berechnen; das `tidyverse` ist für die Arbeit mit Tabellen ausgelegt, nicht für einzelne Werte (oder Vektoren).



Die Abweichung von `prob_null` in der Stichprobe beträgt:



```{r}
abw_stipro <- abs(propdach - prob_null)
abw_stipro
```


Diese Abweichung fügen wir bei jeder simulierten Stichprobe hinzu:

```{r}
Nullvtlg <-
  Nullvtlg %>% mutate(abw = abs(prop - prob_null))
```



```{r}
pvalue_simu1 <- prop( ~ (abw >= abw_stipro),
                 data = Nullvtlg)
pvalue_simu1
```


### Große Stichprobe

Für die große Stichprobe analog:


```{r}
set.seed(1896) 
Nullvtlg2 <- do(10000) * rflip(n = n2, prob = prob_null)

Nullvtlg2 <-
  Nullvtlg2 %>% mutate(abw = abs(prop - prob_null))

pvalue_simu2 <- prop( ~ (abw >= abw_stipro),
                 data = Nullvtlg2)
pvalue_simu2
```



## $\chi^2$-Test

Man beachte, dass der $\chi^2$-Test nur zweiseitig testet.


### Kleine Stichprobe

```{r}
success_freq <- table(d$success)
success_freq
```


```{r}
success_prob <- c(1 - prob_null, prob_null)
```



```{r}
pvalue_chi1 <- chisq.test(x = success_freq,
           p = success_prob
           )$p.value 

pvalue_chi1
```


### Große Stichprobe

```{r}
pvalue_chi2 <- 
  chisq.test(x = table(d2$success),
             p = success_prob)$p.value

pvalue_chi2
```



## Binomialtest


### kleine Stichprobe

```{r}
pvalue_binom1 <- binom.test(x = x,
                            n = n,
                            p = prob_null)$p.value
pvalue_binom1
```



Von Hand:


$E(X) = n * 0.5$

```{r}
E_X <- n * 1/3
E_X
```


$Var(X) = n * 0.25$

```{r}
Var_X = n * 0.25
```



Bei genügend großen Stichproben ist $X$ normalverteilt, daher lässt sich ein z-Wert berechnen:


```{r}
z <- (x - E_X) / sqrt(Var_X)
z
```


Also beträgt der p-Wert:


```{r}
1 - pnorm(q = z)
```




### Große Stichprobe

```{r}
pvalue_binom2 <- binom.test(x = x2,
                            n = n2,
                            p = prob_null)$p.value
pvalue_binom2
```




## Logistische Regression


```{r}
glm1 <- glm(success ~ 1, 
            data = d,
            family = "binomial")
glm1 %>% summary()

```


```{r}
predict(glm1, type = "response", 
        newdata = NULL)
```


Allerdings bezieht sich der p-Wert hier auf eine Referenz-Wahrscheinlchkeit von $p_0 = 1/2$, so dass der p-Wert nicht vergleichbar ist mit den obigen Analysen.


## Zwischen-Fazit

Da finden sich doch ein paar Abweichungen im p-Wert; die Tests testen nicht (ganz) das Gleiche. Möglich, dass die Voraussetzungen jeweils nicht (ausreichend) erfüllt sind.




# Konfidenz-Intervall

## Simulationsbasierte Inferenz (SBI)

```{r}
set.seed(1896)
Bootvtlg <- do(1e4) * prop( ~success, 
                            data = resample(d), 
                            success = 1)
glimpse(Bootvtlg)
```


Die SD der Bootverteilung beträgt:

```{r}
sd_boot <- sd(Bootvtlg$prop_1)
sd_boot
```



```{r}
ggplot(Bootvtlg) +
  aes(x = prop_1) +
  geom_bar()
```

Wie man sieht, ist die Verteilung (annähernd) normalverteilt. D.h. man kann die Das Konfidenzintervall über die Näherung der Normalverteilungsquantile berechnen (s.u.).


KI-Grenzen:

```{r}
ci95 <- quantile( ~ prop_1, data = Bootvtlg, probs = c(0.025, 0.975))

ci95
```

Multipliziert man diese Anteilswerte mit $n$, so bekommt man:



```{r}
ci95[1]*n
ci95[2]*n
```



Anders gesagt liegen die Intervallgrenzen bei `r ci95[1]*n` und `r ci95[2]*n`.


## $\chi^2$-Test


Mit dem $chi^2$-Test kann man kein Konfidenzintervall für den Anteil bekommen, da die Prüfgröße nicht der Anteil ist.


### Chi Square Bootstrap


Man könnte höchstens, wenn man unbedingt will, Die $\chi^2$-Prüfgröße bootstrappen...


Also los:


```{r}
chi_boot <- do(10000) * {chisq.test(x = table(resample(d$success)),
                                  p = success_prob
) %>% extract2("statistic")}
```


```{r}
glimpse(chi_boot)
```


```{r}
ggplot(chi_boot, aes(x = X.squared)) +
  geom_histogram(bins = 20)
```


### Chi p-value Bootstrap

In gleicher Manier kann man den p-Wert als Zufallsvariable auffassen und bootstrappen:



```{r}
chi_p_boot <- do(10000) * {chisq.test(x = table(resample(d$success)),
                                  p = success_prob
) %>% extract2("p.value")}
```


```{r}
glimpse(chi_p_boot)
```


```{r}
ggplot(chi_p_boot, aes(x = result)) +
  geom_histogram()
```


## Binomial-Test, Näherung über Normalverteilung


Wie in der SBI gesehen, ist die Stichprobenverteilung bzw. die Bootstrapverteilung annähernd normalverteilt und zwar mit folgenden Parametern:

```{r}
mue_boot <- x
sigma_boot <- sqrt(Var_X)
```

Dieser theoretische Wert entspricht gut dem durch die Simulation gefundenen Wert:

```{r}
sd_boot*n
```


## Logistische Regression



Das kann man so machen ([Quelle](https://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression)):


```{r}
ci_glm1 <- predict(glm1, type = "response", se.fit = TRUE)
ci_glm1
```


Da wir keinen Prädiktor haben, wird für alle Fälle der gleiche Wert vorhergesagt.

Nun "bauen" wir das KI basierend auf der Annahme einer Normalverteilung:


```{r}
critval <- 1.96 ## approx 95% CI
upr <- ci_glm1$fit + (critval * ci_glm1$se.fit)
lwr <- ci_glm1$fit - (critval * ci_glm1$se.fit)

lwr[1]
upr[1]

```


Das sind die Anteilswerte; multiplizieren wir diese mit $n$:


```{r}
lwr[1]*n
upr[1]*n
```


Also liegen die Intervallgrenzen etwa bei `r round(lwr[1]*n)` (unten) bzw. `r upr[1]*n` (oben).


## Zwischen-Fazit

Die Werte der Konfidenzintervall-Grenzen sind ziemlich ähnlich; das überrascht nicht, denn die Bootverteilung ist schön normalverteilt; die Binomialmethode basiert ebenfalls auf einer Approximation der Normalverteilung. Daher resultieren hier ähnliche Werte.




# Fazit


Die Bootstrap-Methode funktioniert hier gut; natürlich hat sie auch hier [Fallstricke](https://arxiv.org/abs/1411.5279).

Der p-Wert unterscheidet sich je Methode, weil die Signifikanztests unterschiedliche Prüfgrößen berechnen, sprich verschiedene Dinge berechnen. 

# Reproducibility

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
session_info()
```
