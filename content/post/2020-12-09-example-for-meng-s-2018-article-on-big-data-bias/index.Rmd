---
title: Example for Meng's 2018 article on big data bias
author: Sebastian Sauer
date: '2020-12-09'
slug: example-for-meng-s-2018-article-on-big-data-bias
draft: false
categories:
  - rstats
tags:
  - big data
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
---



```{r knitr-setup, echo = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```



# Load packages

```{r load-libs, message = FALSE, warning = FALSE}
library(tidyverse)  # data wrangling
```


# Motivation

My colleague, [Karsten LÃ¼bke](https://twitter.com/luebby42), first grade statistician, pointed me out to a paper ...

In 2018, the statistican Meng wrote a paper about biases in big data [see here](https://projecteuclid.org/euclid.aoas/1532743473). In a nutshell, he argues that non-random samples will be worse when data is larger. Yes, you heard correctly: *Big data is bad data, if not randomly drawn*. That's bold a claim.


# Computing the effective sample size in the 2016' US federal elections

There seem to have been approx. 200 Mio. voters in the 2016 US elections, according to [this source](https://www.politico.com/story/2016/10/how-many-registered-voters-are-in-america-2016-229993); let's take that value for $N$.


Let's follow Meng and take 2 Mio as the sample size $n$ of a survey tapping into the question "who you gonna vote for?".

Let's further assume that Trump voters are a bit ... shyer than Clinton's voters. Say, a given Trump voter is (on average) 0.01% more reluctant to disclose her opinion (that she prefers Trump over Clinton); call this value $d$  (`delta`). Note that that is only a very tiny correlation. Very, very tiny.

```{r}
n <- 2e06
N <- 2e08
delta <- .001
```


```{r}
n_eff <- 4 * (n/N)^2 * (1/delta)^2
n_eff
```


We would end up at a sample size of $n=400$! That's flippingly small.


# Conclusion

I've not yet digested Meng's paper in detail, but it appears well-founded. Given that the conclusions are solid, it means that "big data is bad data", in the most cases. The problem is not big data per se but rather the fact that it is not random data. All else beside random data is ... not really garbish, but of much, much lower quality (and hence information) compared to a real random sample. Of course this problem applies to all other kind of observational research as well.



# Further reading

See [this post](https://civilstat.com/2018/10/mengs-big-data-paradox-and-an-extreme-example/) for an easy to read introduction.




# Reproducibility

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
devtools::session_info()


