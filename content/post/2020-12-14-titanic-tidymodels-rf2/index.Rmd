---
title: 'titanic-tidymodels: boost'
author: Sebastian Sauer
date: '2020-12-14'
slug: titanic-tidymodels-boost
categories:
  - rstats
tags:
  - tutorial
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
---



```{r knitr-setup, echo = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```



```{r load-libs, message = FALSE, warning = FALSE}
library(tidyverse)  # data wrangling
library(tidymodels)  # modelling
library(broom)  # tidy model output
library(skimr)  # overview on descriptives
library(parallel)  # multiple cores -- unix only
```


# Objective

Predicting the survival in the Titanic disaster. We'll be using a tidymodels approach.


# Setup

```{r}
cores <- parallel::detectCores()
cores
```



# Load and prepare data


## Hide details in a function

```{r}
prepare_data <- function(traindata_url = "https://raw.githubusercontent.com/sebastiansauer/Statistiklehre/main/data/titanic/train.csv",
                         testdata_url = "https://raw.githubusercontent.com/sebastiansauer/Statistiklehre/main/data/titanic/test.csv") {


  # import the data:
  train <- read_csv(traindata_url)
  test <- read_csv(testdata_url)
  
 
  # bind both samples into one:
  data_raw <-
    train %>% 
    bind_rows(test)
  
  
  # drop unused variables:
  data <-
    data_raw %>% 
    select(-c(Name, Cabin, Ticket))
  
  
  # convert string to factors:
  data2 <- 
    data %>% 
    mutate(across(where(is.character), as.factor))
  
  # convert numeric outcome to nominal, to indicate classification:
  data2 <- data2 %>% 
    mutate(Survived = as.factor(Survived))
  
  
  return(data2)
}
```





```{r}
data2 <- prepare_data()
```



# Split data into train and test

```{r}
split_titanic <- initial_time_split(data = data2, prop = 891/1309)
train <- training(split_titanic)
test <- testing(split_titanic)
```


# Define recipe

```{r}

titanic_recipe <- 
  
  # define model formula:
  recipe(Survived ~ ., data = train) %>%
  
  # Use "ID" etc as ID, not as predictor:
  update_role(PassengerId, new_role = "ID") %>%   

  # impute missing values:
  step_knnimpute(all_predictors(), neighbors = 3) %>%  
  
  # convert character and factor type variables into dummy variables:
  step_dummy(all_nominal(), -all_outcomes()) %>%   
  
  # exclude near zero variance predictors:
  step_nzv(all_predictors()) %>%  
  
  # exclude highly correlated vars:
  step_corr(all_predictors()) %>% 
  
  # center (set mean to zero):
  step_center(all_predictors(), -all_outcomes()) %>%  
  
  # set sd=1 
  step_scale(all_predictors(), -all_outcomes())  
```


# Define model



```{r def-boost-mod}
boost_mod <- 
  boost_tree(mtry = tune(), 
             min_n = tune(), 
             learn_rate = tune(),
             tree_depth = tune()) %>% 
  set_engine("xgboost", 
             num.threads = cores) %>% 
  set_mode("classification")
```



Translate to `xgboost()` specs:


```{r}
translate(boost_mod)
```



# Define cross validation scheme


```{r vfold-cv}
train_cv <- vfold_cv(train, v = 10)
```



Parameters that can be tuned:


```{r}
boost_mod %>%    
  parameters()  
```



# Define workflow

```{r}
boost_wf <-
  workflow() %>% 
  add_model(boost_mod) %>% 
  add_recipe(titanic_recipe)
```


# Define analysis and validation (oob) set


```{r}
set.seed(42)
val_set <- validation_split(train, 
                            strata = Survived, 
                            prop = 0.80)
```






# Fit the grid


```{r boost-tune-grid}
set.seed(42)

t1 <- Sys.time()
boost_fit <- 
  boost_wf %>% 
  tune_grid(val_set,
            grid = 100,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
  t2 <- Sys.time()

t2 - t1
```

It may be worthwhile to save the result object to disk, in order to save computation time:

```{r}
saveRDS(boost_fit, file = "boost_fit.rds")
```





# View results


```{r}
boost_fit %>% 
  collect_metrics()
```



```{r}
boost_fit %>% 
  show_best(metric = "roc_auc")
```




```{r}
autoplot(boost_fit)
```




# Get best model

```{r r2f-get-best}
boost_best <- 
  boost_fit %>% 
  select_best(metric = "roc_auc")
boost_best
```




# Final fit (on train data)


```{r boost-finalize-wf}
boost_final_wf <- 
  boost_wf %>% 
  finalize_workflow(boost_best)
```

So, here is the best model:

```{r}
boost_final_wf
```


Let us fit this model to the train data:

```{r boost-fit-final-model}
last_boost_mod <- 
  boost_final_wf %>% 
  fit(data = train)
```


```{r}
last_boost_mod
```





## Fit final workflow (on test data)


```{r}
set.seed(42)
last_boost_fit <- 
  last_boost_mod %>% 
  last_fit(split_titanic)
```






# Predict test data

```{r boost-pred}
boost_preds <- 
last_boost_fit %>% 
  collect_predictions() %>% 
  select(-Survived) %>% 
  select(PassengerID = .row, Survived = .pred_class) 
```


```{r}
glimpse(boost_preds)
```



# Save predictions to disk

```{r eval = TRUE}
boost_preds %>% 
  write_csv(file = "boost_preds.csv")
```





# Reproducibility

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
devtools::session_info()


